{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Yen-Hsin Fang\n",
    "## Section: 01\n",
    "\n",
    "# Lab 8: Exploring Random Forests\n",
    "\n",
    "## Tools\n",
    "\n",
    "#### Libraries:\n",
    "\n",
    "- numpy: for processing\n",
    "- sklearn: for model training  \n",
    "- pandas: for data processing  \n",
    "- rfpimp **version 1.3.7**: for feature importance\n",
    "\n",
    "#### Datasets:\n",
    "\n",
    "Boston housing \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Ignore all future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from types import SimpleNamespace\n",
    "def load_boston(return_X_y=False):\n",
    "    \"\"\"Replacement function for loading in Boston House Prices\"\"\"\n",
    "    df = pd.read_csv('./data/boston_house_prices.csv')\n",
    "    X = df.drop(columns=['MEDV'])\n",
    "    y = df['MEDV'].to_numpy()\n",
    "\n",
    "    if return_X_y:\n",
    "        return X, y \n",
    "    \n",
    "    dataset  = SimpleNamespace(data=X, target=y)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boston():\n",
    "    boston = load_boston()\n",
    "    df = boston.data\n",
    "    y = boston.target\n",
    "    df['y'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  \n",
       "0  396.90   4.98  \n",
       "1  396.90   9.14  \n",
       "2  392.83   4.03  \n",
       "3  394.63   2.94  \n",
       "4  396.90   5.33  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boston = boston()\n",
    "X, y = df_boston.drop('y', axis=1), df_boston['y']\n",
    "y *= 1000 # y is \"Median value of owner-occupied homes in $1000's\" so multiply by 1000\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train random forests of different sizes in terms of number of trees\n",
    "\n",
    "In this section we will see that as we increase the number of trees in the ensemble/forest, we should initially see model bias going down, i.e. the predictions getting better. It will asymptotically approach some minimum error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to train a random forest  that has a single tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Compute the MAE for the training and the testing set, printing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1198.3, test 2987.3\n"
     ]
    }
   ],
   "source": [
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Create a quick loop and run the training and testing cycle above several times to see the variance of the training and testing set errors. You should notice that the test set scores bounce around a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1194.6, test 3290.2\n",
      "MAE train 1181.7, test 3123.5\n",
      "MAE train 1127.7, test 2740.2\n",
      "MAE train 1073.5, test 2866.7\n",
      "MAE train 1141.3, test 2471.6\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rf = RandomForestRegressor(n_estimators=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Increase the number of trees (`n_estimators`) to 2, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1098.8, test 2669.6\n",
      "MAE train 1207.4, test 2702.9\n",
      "MAE train 1186.5, test 2495.1\n",
      "MAE train 1135.6, test 2774.0\n",
      "MAE train 1227.4, test 2534.3\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rf = RandomForestRegressor(n_estimators=2)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the scores don't bounce around as much as they did when you were only training one tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  Why does the MAE score go down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because RF averaging predictions from multiple decision trees. As you increase the number of trees, the variance of the model tends to decrease therefore MAE goes down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Increase the number of trees (`n_estimators`) to 10, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1024.2, test 2076.6\n",
      "MAE train 932.1, test 2113.3\n",
      "MAE train 942.6, test 2051.8\n",
      "MAE train 952.4, test 2132.5\n",
      "MAE train 978.7, test 2035.9\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rf = RandomForestRegressor(n_estimators=10)\n",
    "    rf.fit(X_train, y_train) \n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  What do you notice about the MAE scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It decreases in both training and testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  After running several times, what else do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't bounce that much. But the training time is longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Increase the number of trees (`n_estimators`) to 200, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 866.9, test 1906.7\n",
      "MAE train 853.7, test 1890.6\n",
      "MAE train 845.3, test 1915.7\n",
      "MAE train 853.1, test 1958.5\n",
      "MAE train 851.1, test 1931.1\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rf = RandomForestRegressor(n_estimators=200)\n",
    "    rf.fit(X_train, y_train) \n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  What do you notice about the MAE scores from a single run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE drops in training and testing, but not drop significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Both training and testing error have dropped, but not as significantly as before, even with 200 trees.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Notice that it took a little bit longer to train.  Do the exact same thing again but this time use `n_jobs=-1` as an argument to the `RandomForestRegressor` constructor.\n",
    "\n",
    "This tells the library to use all processing cores available on the computer processor. As long as the data is not too huge (because it must pass it around), it often goes much faster using this argument. It should take less than two seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 854.9, test 1906.7\n",
      "MAE train 844.7, test 1955.7\n",
      "MAE train 850.2, test 1923.0\n",
      "MAE train 856.5, test 1934.5\n",
      "MAE train 838.7, test 1901.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    rf = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train) \n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  What do you notice about the MAE scores from SEVERAL runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAE doesn't bounce that much, having lower varience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The error variance across runs is even lower (tighter).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Examining model size and complexity\n",
    "\n",
    "The structure of a tree is affected by a number of hyperparameters, not just the data. The goal in this section is to see the effect of altering the number of observations per leaf and the maximum number of candidate features per split. Let's start out with a handy function that uses some  support code from rfpimp to examine tree size and depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showsize(ntrees, max_features=1.0, min_samples_leaf=1):\n",
    "    rf = RandomForestRegressor(n_estimators=ntrees,\n",
    "                               max_features=max_features,\n",
    "                               min_samples_leaf=min_samples_leaf,\n",
    "                               n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    n = rfnnodes(rf)                # from rfpimp\n",
    "    h = np.median(rfmaxdepths(rf))  # rfmaxdepths from rfpimp\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:6.1f}, test {mae:6.1f} using {n:9,d} tree nodes with {h:2.0f} median tree height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single tree, we see about 480 nodes and a tree height of around 19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1262.1, test 3202.0 using       477 tree nodes with 16 median tree height\n"
     ]
    }
   ],
   "source": [
    "showsize(ntrees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Look at the metrics for 2 trees and then 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1242.0, test 2743.6 using       952 tree nodes with 18 median tree height\n"
     ]
    }
   ],
   "source": [
    "showsize(ntrees=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train  990.6, test 2161.8 using     4,754 tree nodes with 19 median tree height\n"
     ]
    }
   ],
   "source": [
    "showsize(ntrees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.** Why does the median height of a tree stay the same when we increase the number of trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the height of each individual tree remains unchanged since the construction process for a single tree remains constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "While the number of nodes increases with the number of trees, the height of any individual tree will stay the same because we have not fundamentally changed how it is constructing a single tree.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of increasing min samples / leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Loop around a call to `showsize()` with 10 trees and min_samples_leaf=1..10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 MAE train  980.9, test 2063.0 using     4,770 tree nodes with 18 median tree height\n",
      " 2 MAE train 1145.4, test 2180.6 using     2,222 tree nodes with 16 median tree height\n",
      " 3 MAE train 1382.3, test 2273.6 using     1,422 tree nodes with 14 median tree height\n",
      " 4 MAE train 1596.0, test 2284.4 using     1,032 tree nodes with 12 median tree height\n",
      " 5 MAE train 1783.2, test 2548.4 using       808 tree nodes with 12 median tree height\n",
      " 6 MAE train 1940.5, test 2460.2 using       642 tree nodes with 10 median tree height\n",
      " 7 MAE train 1940.8, test 2316.3 using       578 tree nodes with 10 median tree height\n",
      " 8 MAE train 2008.3, test 2531.1 using       500 tree nodes with  9 median tree height\n",
      " 9 MAE train 2106.1, test 2366.7 using       436 tree nodes with  9 median tree height\n",
      "10 MAE train 2162.8, test 2498.3 using       386 tree nodes with  8 median tree height\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(10, min_samples_leaf=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.** Why do the median height of a tree and number of total nodes decrease as we increase the number of samples per leaf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of samples per leaf imposes a constraint that each leaf node must contain at least a certain number of samples. This tends to result in shallower trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  Why does the MAE error increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of observations in a leaf node results in a more general but less accurate prediction, as the average encompasses a larger and potentially more diverse set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "If we include more observations in a single leaf, then the average is taken over more observations. That average is a more general prediction but less accurate.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear from that print out that `min_samples_leaf=1` is the best choice because it gives the minimum validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of reducing max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Do another loop from `max_features` = 4 down to 1, with 1 sample per leaf. (There are 4 total features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 MAE train  960.0, test 2241.7 using     4,752 tree nodes with 20 median tree height\n",
      "12 MAE train 1005.8, test 2177.2 using     4,750 tree nodes with 20 median tree height\n",
      "11 MAE train 1063.1, test 2101.2 using     4,762 tree nodes with 19 median tree height\n",
      "10 MAE train 1010.7, test 2244.3 using     4,800 tree nodes with 20 median tree height\n",
      " 9 MAE train  998.1, test 1974.8 using     4,784 tree nodes with 20 median tree height\n",
      " 8 MAE train  914.7, test 2114.3 using     4,886 tree nodes with 19 median tree height\n",
      " 7 MAE train  960.6, test 1950.7 using     4,878 tree nodes with 18 median tree height\n",
      " 6 MAE train 1006.3, test 2143.4 using     4,784 tree nodes with 19 median tree height\n",
      " 5 MAE train  918.7, test 2020.7 using     4,904 tree nodes with 19 median tree height\n",
      " 4 MAE train 1006.7, test 2352.8 using     4,900 tree nodes with 19 median tree height\n",
      " 3 MAE train 1059.3, test 2535.7 using     4,946 tree nodes with 20 median tree height\n",
      " 2 MAE train 1096.9, test 2504.1 using     4,970 tree nodes with 18 median tree height\n",
      " 1 MAE train 1198.0, test 2828.6 using     5,022 tree nodes with 19 median tree height\n"
     ]
    }
   ],
   "source": [
    "p = X_train.shape[1]\n",
    "for i in range(p,0,-1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=10, max_features=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data set, changing the number of candidate features does not change the height of the tree much nor do we see a very clear pattern for the test set error, e.g. the error clearly increasing or decreasing as we decrease the number of candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF prediction confidence\n",
    "\n",
    "A random forest is a collection of decision trees, each of which contributes a prediction. The forest averages those predictions to provide the overall prediction (or takes most common vote for classification). Let's dig inside the random forest to get the individual trees out and ask them what their predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Train a random forest with 10 trees on `X_train`, `y_train`.  Use `for t in rf.estimators_` to iterate through the trees making predictions with `t` not `rf`. Print out the usual MAE scores for each tree predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1276.2, test 3532.4\n",
      "MAE train 1130.0, test 2621.6\n",
      "MAE train 1330.2, test 2960.8\n",
      "MAE train 1370.0, test 3183.3\n",
      "MAE train 1208.2, test 2532.4\n",
      "MAE train 1333.7, test 3253.9\n",
      "MAE train 1101.0, test 3035.3\n",
      "MAE train 1402.2, test 3110.8\n",
      "MAE train 1286.6, test 3148.0\n",
      "MAE train 1186.4, test 2810.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in rf.estimators_:\n",
    "    mae_train = mean_absolute_error(y_train, t.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, t.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it bounces around quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Select any one of the `X_test` rows and print out the predicted rent price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.537    0.       6.2      0.       0.504    5.981   68.1      3.6715\n",
      "    8.     307.      17.4    378.35    11.65  ]] => [21150.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x = X_test.iloc[30,:] # pick single test case\n",
    "x = x.values.reshape(1,-1)\n",
    "print(f\"{x} => {rf.predict(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Now let's see how the forest came to that conclusion. Compute the average of the predictions obtained from every tree. Compare that to the prediction obtained directly from the random forest (`rf.predict(X_test)`). They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.537    0.       6.2      0.       0.504    5.981   68.1      3.6715\n",
      "    8.     307.      17.4    378.35    11.65  ]] => 21150.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Compute the standard deviation of the tree estimates and print that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2542.538101976055"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([t.predict(x) for t in rf.estimators_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "np.std([t.predict(x) for t in rf.estimators_])\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the standard deviation, the more tightly grouped the predictions were, which means we should have more confidence in our answer. \n",
    "\n",
    "Different records will often have different standard deviations, which means we could have different levels of confidence in the various answers. This might be helpful to a bank for example that wanted to not only predict whether to give loans, but how confident the model was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering bootstrap size\n",
    "\n",
    "In this section we will tune one final hyperparameter and see how it affects the model: the `max_samples` which controls the size of the bootstrapped data set used for fitting each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: There are only about 400 training records, change that to 200 and check the error again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 318 ms, sys: 3.19 ms, total: 321 ms\n",
      "Wall time: 325 ms\n",
      "MAE train 848.3, test 1917.1\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200) # don't compute in parallel so we can see timing\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 1.96 ms, total: 211 ms\n",
      "Wall time: 211 ms\n",
      "MAE train 1428.7, test 2026.6\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, max_samples=1/2)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit less accurate, but it's faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  Why is it less accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this randomness can enhance the overall performance of the Random Forest, it may also lead to less accurate predictions for individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Turn off bootstrapping by adding `bootstrap=False` to the constructor of the model. This means that it will subsample rather than bootstrap. Remember that bootstrapping gets about two thirds of the data because of replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 547 ms, sys: 60.3 ms, total: 607 ms\n",
      "Wall time: 149 ms\n",
      "MAE train 0.0$, test 2873.0$\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, bootstrap=False)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happened to the training set error. It got quite a bit lower when we do not do bootstrapping, we are overfitting by a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
